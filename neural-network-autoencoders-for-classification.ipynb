{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":20477,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=\"center\">Semi Supervised Classification using Neural Network AutoEncoders</h1>\n\n## Introduction\n\nThis Semi-Supervised Learning uses unlabelled data to train a model for one class by learning the best possible representation using a neural network. Given the other class is different, it will automatically separate out the other class.\n\n \n## 1. Dataset Preparation\n\nFirst, we will load all the required libraries and load the dataset using pandas dataframe. \n \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"from keras.layers import Input, Dense\nfrom keras.models import Model, Sequential\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nnp.random.seed(203)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T00:36:52.863379Z","iopub.execute_input":"2024-03-11T00:36:52.863894Z","iopub.status.idle":"2024-03-11T00:36:58.147423Z","shell.execute_reply.started":"2024-03-11T00:36:52.863815Z","shell.execute_reply":"2024-03-11T00:36:58.146340Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('../input/creditcardfraud/creditcard.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:26:39.436715Z","iopub.execute_input":"2024-03-11T01:26:39.437162Z","iopub.status.idle":"2024-03-11T01:26:42.744638Z","shell.execute_reply.started":"2024-03-11T01:26:39.437102Z","shell.execute_reply":"2024-03-11T01:26:42.743618Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   Time        V1        V2        V3  ...         V27       V28  Amount  Class\n0   0.0 -1.359807 -0.072781  2.536347  ...    0.133558 -0.021053  149.62      0\n1   0.0  1.191857  0.266151  0.166480  ...   -0.008983  0.014724    2.69      0\n2   1.0 -1.358354 -1.340163  1.773209  ...   -0.055353 -0.059752  378.66      0\n3   1.0 -0.966272 -0.185226  1.792993  ...    0.062723  0.061458  123.50      0\n4   2.0 -1.158233  0.877737  1.548718  ...    0.219422  0.215153   69.99      0\n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>-0.551600</td>\n      <td>-0.617801</td>\n      <td>-0.991390</td>\n      <td>-0.311169</td>\n      <td>1.468177</td>\n      <td>-0.470401</td>\n      <td>0.207971</td>\n      <td>0.025791</td>\n      <td>0.403993</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>1.612727</td>\n      <td>1.065235</td>\n      <td>0.489095</td>\n      <td>-0.143772</td>\n      <td>0.635558</td>\n      <td>0.463917</td>\n      <td>-0.114805</td>\n      <td>-0.183361</td>\n      <td>-0.145783</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>0.624501</td>\n      <td>0.066084</td>\n      <td>0.717293</td>\n      <td>-0.165946</td>\n      <td>2.345865</td>\n      <td>-2.890083</td>\n      <td>1.109969</td>\n      <td>-0.121359</td>\n      <td>-2.261857</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>-0.226487</td>\n      <td>0.178228</td>\n      <td>0.507757</td>\n      <td>-0.287924</td>\n      <td>-0.631418</td>\n      <td>-1.059647</td>\n      <td>-0.684093</td>\n      <td>1.965775</td>\n      <td>-1.232622</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>-0.822843</td>\n      <td>0.538196</td>\n      <td>1.345852</td>\n      <td>-1.119670</td>\n      <td>0.175121</td>\n      <td>-0.451449</td>\n      <td>-0.237033</td>\n      <td>-0.038195</td>\n      <td>0.803487</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:26:48.784230Z","iopub.execute_input":"2024-03-11T01:26:48.784650Z","iopub.status.idle":"2024-03-11T01:26:48.800978Z","shell.execute_reply.started":"2024-03-11T01:26:48.784575Z","shell.execute_reply":"2024-03-11T01:26:48.799713Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0    284315\n1       492\nName: Class, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Autoencoders are a special type of neural network architechures in which the output is same as the input. \nAutoencoders are trained in an unsupervised manner in order to learn the extremely low level representation of the input data.\nThese low level features are then defomred back to project the acutal data.\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T00:49:50.323065Z","iopub.execute_input":"2024-03-11T00:49:50.323366Z","iopub.status.idle":"2024-03-11T00:49:50.328705Z","shell.execute_reply.started":"2024-03-11T00:49:50.323321Z","shell.execute_reply":"2024-03-11T00:49:50.328098Z"}}},{"cell_type":"code","source":"non_fraud = data[data['Class'] == 0].sample(1000)\nfraud = data[data['Class'] == 1]\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['Class'],axis=1).values\ny = df['Class'].values","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:29:12.656857Z","iopub.execute_input":"2024-03-11T01:29:12.657925Z","iopub.status.idle":"2024-03-11T01:29:12.779640Z","shell.execute_reply.started":"2024-03-11T01:29:12.657823Z","shell.execute_reply":"2024-03-11T01:29:12.778481Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# input layer\ninput_layer = Input(shape = (X.shape[1],))\n\n# encoding part\nencoded = Dense(100, activation='tanh', activity_regularizer = regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50, activation='relu')(encoded)\n\n# decoding part\ndecoded = Dense(50, activation = 'tanh')(encoded)\ndecoded = Dense(100, activation = 'tanh')(decoded)\n\n# output layer\noutput_layer = Dense(X.shape[1], activation='relu')(decoded)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:29:43.605001Z","iopub.execute_input":"2024-03-11T01:29:43.605416Z","iopub.status.idle":"2024-03-11T01:29:43.706246Z","shell.execute_reply.started":"2024-03-11T01:29:43.605339Z","shell.execute_reply":"2024-03-11T01:29:43.704283Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"autoencoder = Model(input_layer,output_layer)\nautoencoder.compile(optimizer='adadelta',loss='mse')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:29:49.795843Z","iopub.execute_input":"2024-03-11T01:29:49.796285Z","iopub.status.idle":"2024-03-11T01:29:49.831193Z","shell.execute_reply.started":"2024-03-11T01:29:49.796213Z","shell.execute_reply":"2024-03-11T01:29:49.830384Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"x = data.drop(['Class'],axis=1).values\ny = data['Class'].values\n\nx_scale = preprocessing.MinMaxScaler().fit_transform(x)\nx_norm, x_fraud = x_scale[y==0], x_scale[y==1] ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:33:19.061664Z","iopub.execute_input":"2024-03-11T01:33:19.062677Z","iopub.status.idle":"2024-03-11T01:33:19.509397Z","shell.execute_reply.started":"2024-03-11T01:33:19.062589Z","shell.execute_reply":"2024-03-11T01:33:19.508386Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"The beauty of this approach is that we do not need too many samples for learning the good representations. We will use only 2000 rows of non fraud to train the autoencoder. Additionally, we do not need to run this model for a larger number of epochs.","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:16:06.024643Z","iopub.execute_input":"2024-03-11T01:16:06.025106Z","iopub.status.idle":"2024-03-11T01:16:06.034497Z","shell.execute_reply.started":"2024-03-11T01:16:06.025028Z","shell.execute_reply":"2024-03-11T01:16:06.033144Z"}}},{"cell_type":"code","source":"autoencoder.fit(x_norm[0:2000],x_norm[0:2000],\n               batch_size = 256, \n               epochs = 10,\n               shuffle = True,\n               validation_split = 0.20)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:33:30.565002Z","iopub.execute_input":"2024-03-11T01:33:30.565433Z","iopub.status.idle":"2024-03-11T01:33:32.049447Z","shell.execute_reply.started":"2024-03-11T01:33:30.565357Z","shell.execute_reply":"2024-03-11T01:33:32.048627Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Train on 1600 samples, validate on 400 samples\nEpoch 1/10\n1600/1600 [==============================] - 1s 356us/step - loss: 0.7302 - val_loss: 0.4526\nEpoch 2/10\n1600/1600 [==============================] - 0s 14us/step - loss: 0.4062 - val_loss: 0.2720\nEpoch 3/10\n1600/1600 [==============================] - 0s 14us/step - loss: 0.2591 - val_loss: 0.1909\nEpoch 4/10\n1600/1600 [==============================] - 0s 15us/step - loss: 0.2000 - val_loss: 0.1592\nEpoch 5/10\n1600/1600 [==============================] - 0s 14us/step - loss: 0.1676 - val_loss: 0.1671\nEpoch 6/10\n1600/1600 [==============================] - 0s 14us/step - loss: 0.1767 - val_loss: 0.1384\nEpoch 7/10\n1600/1600 [==============================] - 0s 13us/step - loss: 0.1388 - val_loss: 0.1272\nEpoch 8/10\n1600/1600 [==============================] - 0s 16us/step - loss: 0.1518 - val_loss: 0.1199\nEpoch 9/10\n1600/1600 [==============================] - 0s 15us/step - loss: 0.1282 - val_loss: 0.1218\nEpoch 10/10\n1600/1600 [==============================] - 0s 18us/step - loss: 0.1384 - val_loss: 0.1160\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7c8894f6dd30>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Obtain the latent represntations","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:42:13.116736Z","iopub.execute_input":"2024-03-11T01:42:13.117198Z","iopub.status.idle":"2024-03-11T01:42:13.122291Z","shell.execute_reply.started":"2024-03-11T01:42:13.117146Z","shell.execute_reply":"2024-03-11T01:42:13.121066Z"}}},{"cell_type":"code","source":"hidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:43:20.200222Z","iopub.execute_input":"2024-03-11T01:43:20.200595Z","iopub.status.idle":"2024-03-11T01:43:20.219630Z","shell.execute_reply.started":"2024-03-11T01:43:20.200543Z","shell.execute_reply":"2024-03-11T01:43:20.218692Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"norm_hid_rep = hidden_representation.predict(x_norm[:3000])\nfraud_hid_rep = hidden_representation.predict(x_fraud)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:44:21.360442Z","iopub.execute_input":"2024-03-11T01:44:21.360877Z","iopub.status.idle":"2024-03-11T01:44:21.461030Z","shell.execute_reply.started":"2024-03-11T01:44:21.360821Z","shell.execute_reply":"2024-03-11T01:44:21.459801Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"norm_hid_rep.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:47:45.392093Z","iopub.execute_input":"2024-03-11T01:47:45.392823Z","iopub.status.idle":"2024-03-11T01:47:45.402081Z","shell.execute_reply.started":"2024-03-11T01:47:45.392717Z","shell.execute_reply":"2024-03-11T01:47:45.400794Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(3000, 50)"},"metadata":{}}]},{"cell_type":"code","source":"rep_x = np.append(norm_hid_rep,fraud_hid_rep,axis=0)\ny_n = np.zeros(norm_hid_rep.shape[0])\ny_f = np.ones(fraud_hid_rep.shape[0])\nrep_y = np.append(y_n,y_f)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T01:52:30.076734Z","iopub.execute_input":"2024-03-11T01:52:30.077173Z","iopub.status.idle":"2024-03-11T01:52:30.085202Z","shell.execute_reply.started":"2024-03-11T01:52:30.077119Z","shell.execute_reply":"2024-03-11T01:52:30.083563Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Simple Linear Classifier","metadata":{}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(rep_x,rep_y,test_size=0.25)\nlr = LogisticRegression(solver='lbfgs').fit(train_x,train_y)\npred_y = lr.predict(val_x)\nprint(classification_report(val_y,pred_y))\nprint(accuracy_score(val_y,pred_y))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T02:10:45.077232Z","iopub.execute_input":"2024-03-11T02:10:45.077678Z","iopub.status.idle":"2024-03-11T02:10:45.267360Z","shell.execute_reply.started":"2024-03-11T02:10:45.077628Z","shell.execute_reply":"2024-03-11T02:10:45.265985Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         0.0       0.98      1.00      0.99       755\n         1.0       1.00      0.90      0.95       118\n\n   micro avg       0.99      0.99      0.99       873\n   macro avg       0.99      0.95      0.97       873\nweighted avg       0.99      0.99      0.99       873\n\n0.9862542955326461\n","output_type":"stream"}]}]}